{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9e5311",
   "metadata": {},
   "source": [
    "# 1. Generate model thought traces for evaluation of diagnostic reasoning of 4 frontier LLMs (n = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be0d44",
   "metadata": {},
   "source": [
    "## Part A. Load in combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68aa4aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "case_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "vignette",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "diagnosis",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "difficulty",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "diagnosis_dsm",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "diagnosis_icd",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "reasoning",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "dde16061-1d58-4317-a90e-f7072430d8a2",
       "rows": [
        [
         "0",
         "2",
         "A black male in his early 20s with a remote history of depression as an adolescent and current cannabis use presented following an assault while incarcerated. He was found unconscious with mental status changes and brought to the hospital. Initial examination and work-up revealed right-sided periorbital edema and ecchymosis. The patient was arousable to noxious stimuli but would quickly return to sleep and was not interactive during multiple examination attempts. The emergency department staff believed there was a volitional component to his lack of participation in the exam, as he would push away from providers as they examined him or lie unresponsive. At one point, the patient was able to ambulate briefly while in the emergency department (although he ran into several objects) and tolerated an oral intake challenge with juice. Over the course of 5 days, the patient was returned to the emergency department by the facility four more times due to concerns that his mental status was not improving. The facility reported the patient was unable to attend to his ADLs. He was seen to be withdrawn, not eating or drinking, and toileting on himself or the floor. Additionally, guards (as he was incarcerated for part of his hospital admission) reported that he would occasionally speak to family members and staff. However, when he was offered paperwork to sign himself out of jail with an ankle monitor, he only gazed at the officer without speaking.\n\nUrine toxicology was positive for THC and a negative blood alcohol level. He was found to have ketones consistent with starvation ketosis. Lumbar puncture revealed CSF cell count with six total nucleated cells present, <2,000 RBC, 54 mg/dL glucose, and 39 mg/dL protein. Meningitis/encephalitis panel for 14 common viral and bacterial pathogens and antibody testing for West Nile Virus was negative. CT of the head showed no fractures of the face/vertebra and was negative for other acute findings. EEG was read as consistent with a normal drowsy state. MRI of the brain, both with and without contrast, showed a cerebellar developmental venous anomaly but no acute findings. A neurology consult was obtained. His neurologic exam was noted to be variable, with poor participation despite repeated prompting, making full examination difficult. He responded to pain equally in all extremities, demonstrated no abnormal movements, had 2+ reflexes symmetrically, and normal pupillary response. Neurology ultimately felt that his LP, MRI, and EEG were unremarkable, and his symptomatology could be explained by postconcussive symptoms.\n\nOnce the patient was admitted, he was initially treated with a trial of Modafinil for suspected postconcussive syndrome, to which there was no response. On Day 6 of his hospitalization, when the psychiatric consultation service reconsidered his disordered behavior, psychomotor retardation, and speech with consideration for the presence of catatonia, the question of “volitional” inactivity took on a new meaning. There were numerous catatonia signs present to variable degrees: mutism, decreased oral intake, negativism, withdrawal, and decreased spontaneous movement. No automatic obedience, ambitendency, or rigidity was appreciated. ",
         "Catatonia",
         "case_reports_in_psychiatry",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "4",
         "A 12-year-old male with no specific birth or medical history became concerned about lower abdominal distention and began reducing his food intake 3 months prior to consultation. He exercised obsessively according to his food intake. Although he was considerably losing weight, he commented that his belly was fat and did not want it to get any fatter. Body image disturbances and anxiety regarding being overweight were observed. During his initial consultation, he was very lean, with a body mass index (BMI) of 11.5 kg/m2. He also had sinus bradycardia with a heart rate of approximately 40 beats/min. We attempted to improve his weight with enteral nutritional supplements while identifying underlying diseases by head magnetic resonance imaging (MRI) and abdominal ultrasound. However, he became disoriented and was emergently hospitalized 7 days after his first visit. On admission, he was 145 cm tall, weighed 20.9 kg (BMI: 9.9 kg/m2), and presented with impaired consciousness with Glasgow Coma Scale (GCS) E3V4M6, bradycardia with a heart rate of about 40 beats/min, and slow breathing with a respiratory rate of about 10 cycles/min. Blood gas analysis and blood test revealed hypercapnia, hypoglycemia, and elevated levels of creatinine, blood urea nitrogen, and transaminases. His movements were slow but not paralytic and his cranial nerves were normal. ",
         "Anorexia nervosa with focal cortical dysplasia",
         "case_reports_in_psychiatry",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "13",
         "We report on a 35-year-old female with a history of schizoaffective disorder, bipolar type, and cocaine use disorder, with multiple previous psychiatric admissions, who was brought to our hospital's crisis unit after her parents called police, complaining that she had become aggressive and was “acting delusional” during an argument. According to her parents, the patient had demanded a DNA test to prove that her parents were not her biological parents and that her son was not her biological son. One week prior to admission, the patient's parents had reported her missing when she “disappeared” from their home soon after being discharged from our hospital following a similar presentation. Prior to admission, the patient's medications included lamotrigine, clonazepam, and long-acting injectable aripiprazole; however, her adherence to treatment recommendations was poor.\n\nAlthough her parents claimed that she resided with them and her 2-year-old son, the patient endorsed a recent history of homelessness lasting several months. The patient said that during this period of homelessness, she had been sexually assaulted by a stranger in his hotel room. Collateral revealed that per court order, the patient's continued custody of her son was contingent on her adherence to psychotropic medications and inpatient drug rehabilitation. The patient admitted to using recreational drugs frequently over the past 6 months, including crack cocaine, which she stated she had used during the 1-week interval between hospitalizations.\n\nDuring her initial mental status examination, the patient was poorly groomed, uncooperative, intrusive, and irritable. Her speech was pressured, and she described her recent sleep as poor. She commented that she intended to become pregnant and to sell the baby to an agency for $6,000. She made religious statements about “being fruitful and multiplying” and admitted to recently engaging in hypersexual behavior, explaining that “I have boyfriends on the streets.” She underwent testing for sexually transmitted infections, with reassuring results. She declined voluntary inpatient psychiatric admission and was subsequently screened and committed. We discontinued clonazepam at the time of admission, and we restarted the patient on lamotrigine, adding oral aripiprazole to supplement her long-acting injectable aripiprazole.\n\nDuring her hospitalization, the patient faced several barriers to efficient psychiatric stabilization. First among these was her insistence on refusing any medications that might cause weight gain, including olanzapine. She endorsed allergies to haloperidol, risperidone, lurasidone, and lithium, with symptoms such as “anxiety,” “eye rolling,” and “facial twitching” in response to these drugs. Because she was adherent to oral lamotrigine and aripiprazole, we chose not to seek treatment over objection to administer a stronger antipsychotic medication that would also serve as an effective mood stabilizer. On hospital Day 4, the patient appeared virtually in family court, where she received an ultimatum to cooperate with treatment recommendations in order to maintain custody of her son. Nonetheless, she remained adamant about not being placed on “weight gainers,” stating that “I can have nine more babies. I want to be sexy.” On hospital Day 6, she tested positive for COVID-19 and was quarantined for 10 days on a medical floor. Her respiratory symptoms included cough and rhinorrhea, and she continued to display manic symptoms, including elated affect, hyperverbality, grandiosity, religious preoccupation, and inappropriate laughter. She exhibited behaviors suggestive of sexual preoccupation, such as raising her shirt during interviews to reveal a protuberant abdomen and repeatedly endorsing a desire to “stay sexy.”\n\nAfter her quarantine, the patient returned to the psychiatric unit with her manic symptoms unresolved. On hospital Day 17, she requested a pregnancy test due to concerns that she might have become pregnant because she “had sex with a bunch of guys because I was homeless.” The patient seemed reassured when we explained that her admission pregnancy test was negative. We ordered a new quantitative β-hCG test, which was likewise negative. On Day 18, the patient received a maintenance dose of long-acting injectable aripiprazole. On Day 19, she complained of abdominal and back pain, stating “I've been in labor since last night.” She elaborated that “I can feel the head in my crotch” and “when I lay on my back, I feel the little hands and feet.” She also stated that her “water broke” and her “mucus plug broke.” The patient exhibited several physical signs and symptoms of pregnancy, including a distended abdomen, intermittent “pulsating” bilateral lower abdominal cramps, and sensations of fetal movement. In addition, she endorsed amenorrhea for the past year, except for some “mild spotting” three weeks previously. Convinced that her two recent pregnancy tests were “false negatives” and frustrated that members of the treatment team did not believe she was pregnant, she threatened to sue the hospital for failing to provide obstetric care. The patient lamented that “you all think I'm crazy, but I'm actually pregnant.” At this time, she displayed an intensifying religious preoccupation and stated that she was able to communicate with angels and “see Jesus.” She proclaimed that she herself was a prophet who could interpret the “auras” of hospital staff.\n\nThe patient agreed to a pelvic exam, which revealed a distended, nontender abdomen. The uterus was nongravid. The patient received acetaminophen and heating pads, and we ordered a transabdominal ultrasound to better assess the etiology of her abdominal pain and in the hopes of resolving her condition via additional evidence of nonpregnancy. At this time, the patient began to refuse all previously prescribed medications because she believed these would harm her “baby.” She did, however, agree to begin fluphenazine elixir, which we prescribed to address worsening psychosis.\n\nWhen she arrived in the ultrasound suite, the patient offered the radiology technologist four million dollars “if you get this baby right.” She became euphoric while viewing the ultrasound monitor, exclaiming “That's a head, and that's a penis!” Her speech was pressured, and she perseverated on religious themes, requesting that all non-Christian staff leave the suite and instructing staff to “resist the Devil and he will flee from you!” She also demonstrated sexual preoccupation, recalling an experience in which she had “danced after I had sex” and exclaiming about the size of her “baby's penis,” predicting that he would be “sexy like his daddy.” After the procedure, the patient happily told fellow patients that she would soon deliver another child. She said, “I have been having contractions for 19 hr. When the results come back and they find out I am not crazy, they will finally send me to maternity.”\n\nAn on-call psychiatry resident presented the radiology report to the patient on hospital Day 20. She emphatically refuted the report, articulating a persecutory delusion of a conspiracy waged against her by the treatment team. She stated that the radiologist must have been “paid off” to switch her ultrasound with that of another patient. She continued to refuse medications that she felt might harm her “baby”; however, she agreed to an increase in her dose of fluphenazine. Following this dose increase, her affect remained labile, with frequent swings between euphoria and irritability, but she ceased making comments indicative of religious and sexual preoccupation. As our hospital is a short-term care facility, we decided to refer the patient to a higher level of care facility. On the day of her discharge, the patient told a nurse that “I don't think I'm pregnant anymore; it's all the shit I have inside me,” referring to her constipated bowels.",
         "Pseudocyesis",
         "case_reports_in_psychiatry",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "14",
         "The patient is a 58-year-old woman with a psychiatric history of generalized anxiety disorder (GAD), major depressive disorder (MDD), post-traumatic stress disorder (PTSD), dyssomnia, tobacco use disorder, and alcohol use disorder in remission. Her tobacco use disorder began at age 22, and she smoked two packs of cigarettes daily. At age 46, she decreased her tobacco use to smoking one and a half packs of cigarettes daily. Two years later, she further decreased her tobacco use to smoking one pack of cigarettes daily.\n\nHer alcohol use disorder began at age 38, and she drank a pint (473 mL) of hard liquor daily. At age 45, she decreased her alcohol use to half a pint (236.5 mL) of hard liquor daily. The following year, she decreased her alcohol use to only weekends. Two years later, she used alcohol very occasionally. At age 49, her husband passed, and her alcohol use increased to a fifth (750 mL) of gin daily. She was hospitalized twice that year for chronic pancreatitis and quit drinking after the second hospitalization.\n\nShe struggled with worsening GAD, MDD, and PTSD since age 39. At age 46, she was prescribed citalopram but did not tolerate it. At age 49, she began regularly seeing outpatient psychiatry. She was prescribed sertraline and prazosin. A few months later, her MDD and PTSD symptoms significantly improved. However, her GAD worsened, and she had more frequent exacerbations. Her sertraline and prazosin dosages were increased. She was also prescribed hydroxyzine but did not tolerate it. Additionally, she received grief therapy as recommended. Her MDD and PTSD symptoms improved significantly more, but her GAD continued to be uncontrolled. Intermittently, she would be out of her medications, and her MDD and GAD would worsen. Her PTSD was stable, even when out of medication.\n\nAlso, the patient was prescribed melatonin for her dyssomnia at age 50. Her dyssomnia was uncontrolled despite increasing the melatonin dosage. She did not tolerate trazodone. The patient was then lost to follow-up with outpatient psychiatry for 2 years and returned at age 53. Since then, she has had regular follow-up appointments with outpatient psychiatry.\n\nWhen she returned to outpatient psychiatry at age 53, she had been off of all psychiatric medications. Her MDD was partially stable. Her PTSD was stable. Her GAD and dyssomnia were uncontrolled. She had resumed drinking small amounts of alcohol on occasion, about once a month, and she continued to smoke one pack of cigarettes daily. Her sertraline was resumed, and she was started on zolpidem. Her dyssomnia improved with zolpidem. Eventually, her sertraline was switched to paroxetine. After trialing multiple medications, she was started on diazepam for a short period for her uncontrolled anxiety. Psychotherapy was also recommended, but she did not take part in it. In April 2020, at age 55, her outpatient psychiatry appointments were converted from in-person to telehealth due to COVID-19 health regulations.\n\nAt age 49, the patient first noted fatigue and poor memory. At a follow-up appointment a few months later, she continued to report poor memory. A year later, she reported worsening memory, resulting in losing her keys and forgetting important dates. After that, she did not report fatigue or memory concerns for several years.\n\nIn August 2021, at age 56, the patient had mild COVID-19, which did not require hospitalization. Since then, for a period of over 1.5 years, she has reported persistent symptoms associated with LC-19. At her following appointment in December 2021, she noted fatigue, breathlessness, ageusia, poor appetite, weight loss, and “not feeling right.” In January 2022, she also reported poor memory, increased confusion, and being easily distracted. She was concerned she had an unnoticed stroke that was causing these symptoms. In February 2022, these symptoms persisted, and she reported left-side weakness. In April 2022, she also reported anosmia, “brain fog,” and feeling disorganized. She stated she could not describe her cognitive symptoms with words. In August 2022, she also reported generalized weakness, loss of balance, dizziness, body temperature changes, and increased disorganization. She stated she couldn't go to walk-in psychotherapy due to her disorganization.\n\nAt this time, she also began experiencing significant impairments in her daily functioning. She reported difficulties with instrumental activities of daily living, such as cooking. In September 2022, she also reported trouble with word finding. In October 2022, she also reported concern regarding her sensation of touch and vision. She reported that things felt different, and she thought her vision “easily missed things.” For example, she would not have initially seen a trash bag and would suddenly notice it was there. In January 2023, she also expressed feeling unable to drive, although once she was in the car, she could complete the task. In March 2023, she noted not being able to feel her left hand and believed she may have had a stroke to cause this. Other than fatigue and difficulty with memory, the patient did not express any of these symptoms or concerns prior to August 2021.\n\nHer social isolation, due to her symptoms, further exacerbated her distress. She reported excessive anxiety regarding the uncertain etiology of her symptoms. She frequently expressed concern that her physicians were not doing enough to diagnose and treat her. She requested a comprehensive work-up and hospital admission. She did not meet the criteria for hospital admission and was not admitted.",
         "Illness anxiety disorder (IAD)",
         "case_reports_in_psychiatry",
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "15",
         "The patient is a 20-year-old single man. He is unemployed and has a history of polysubstance abuse and family problems. For the past 2 years, he has used alcohol, cocaine, and medical substance (Lyrica and other medications). He has a history of social anxiety with biological symptoms, which are unresponsive to medication. He expressed low self esteem and feelings of worthlessness. He was pessimistic about the future, saying, “I don't see anything ahead for me.” He described a passive death wish, but denied having any active suicidal thoughts. He described feeling fast heartbeats, blushing, trembling, sweating, trouble catching his breath, dizziness, and panic complaints, especially when talking to people and women in specific. He also described being unable to carry out a conversation with others, feelings of being watched and laughed at, and believing that he is ugly looking. He described having these complaints since adolescence. The patient has been an addict for the past 2 years. He started with Marijuana and alcohol, and then he tried several medical drugs.\nThe patient talked about various upsetting memories of instances that took place during his childhood and up to his late teenage years. This included memories, such as not being given a choice or participating in decision making, his father's absence from the house due to travel, followed by the discovery of his marriage to another woman and the presence of children. His family's upbringing style is firm, authoritarian on the part of the mother, and permissive on the part of the father.",
         "Social anxiety disorder (SAD) and major depressive disorder (MAD)",
         "case_reports_in_psychiatry",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>vignette</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>source</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>diagnosis_dsm</th>\n",
       "      <th>diagnosis_icd</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>A black male in his early 20s with a remote hi...</td>\n",
       "      <td>Catatonia</td>\n",
       "      <td>case_reports_in_psychiatry</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>A 12-year-old male with no specific birth or m...</td>\n",
       "      <td>Anorexia nervosa with focal cortical dysplasia</td>\n",
       "      <td>case_reports_in_psychiatry</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>We report on a 35-year-old female with a histo...</td>\n",
       "      <td>Pseudocyesis</td>\n",
       "      <td>case_reports_in_psychiatry</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>The patient is a 58-year-old woman with a psyc...</td>\n",
       "      <td>Illness anxiety disorder (IAD)</td>\n",
       "      <td>case_reports_in_psychiatry</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>The patient is a 20-year-old single man. He is...</td>\n",
       "      <td>Social anxiety disorder (SAD) and major depres...</td>\n",
       "      <td>case_reports_in_psychiatry</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id                                           vignette  \\\n",
       "0        2  A black male in his early 20s with a remote hi...   \n",
       "1        4  A 12-year-old male with no specific birth or m...   \n",
       "2       13  We report on a 35-year-old female with a histo...   \n",
       "3       14  The patient is a 58-year-old woman with a psyc...   \n",
       "4       15  The patient is a 20-year-old single man. He is...   \n",
       "\n",
       "                                           diagnosis  \\\n",
       "0                                          Catatonia   \n",
       "1     Anorexia nervosa with focal cortical dysplasia   \n",
       "2                                       Pseudocyesis   \n",
       "3                     Illness anxiety disorder (IAD)   \n",
       "4  Social anxiety disorder (SAD) and major depres...   \n",
       "\n",
       "                       source difficulty diagnosis_dsm diagnosis_icd reasoning  \n",
       "0  case_reports_in_psychiatry       None          None          None      None  \n",
       "1  case_reports_in_psychiatry       None          None          None      None  \n",
       "2  case_reports_in_psychiatry       None          None          None      None  \n",
       "3  case_reports_in_psychiatry       None          None          None      None  \n",
       "4  case_reports_in_psychiatry       None          None          None      None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the combined (literature + synthetic) dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"../../datasets/combined/combined_jama.json\"\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    combined = json.load(f)\n",
    "\n",
    "dataset = pd.DataFrame(combined)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b66d5e",
   "metadata": {},
   "source": [
    "## Part B. Create dataset subset for diagnostic reasoning analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ace785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 30 random cases from the combined dataset for diagnostic reasoning analysis and human comparison\n",
    "import random\n",
    "random.seed(1000)  # For reproducibility\n",
    "\n",
    "# Exclude a set of case IDs if needed (seen by clinicians in previous round of evaluation)\n",
    "excluded_case_ids = [48, 81, 90, 142, 166, 168, 176, 177, 44, 43,\n",
    "            113, 114, 131, 132, 152, 156, 21, 27, 50, 89,\n",
    "            110, 123, 153, 175, 1001, 1002, 1010, 1011, 1014, 1019]\n",
    "\n",
    "filtered_combined = [case for case in combined if case['case_id'] not in excluded_case_ids]\n",
    "\n",
    "reasoning_subset = random.sample(filtered_combined, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a444bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported reasoning subset to ../../datasets/reasoning/jama_reasoning_subset.xlsx.\n"
     ]
    }
   ],
   "source": [
    "# Export reasoning subset as Excel spreadsheet to preserve Unicode characters like quotation marks\n",
    "reasoning_subset = pd.DataFrame(reasoning_subset)\n",
    "reasoning_output_path = \"../../datasets/reasoning/jama_reasoning_subset.xlsx\"\n",
    "\n",
    "reasoning_subset.to_excel(reasoning_output_path, index=False)\n",
    "print(f\"Exported reasoning subset to {reasoning_output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1609e01",
   "metadata": {},
   "source": [
    "## Part C. Set up LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c52fb",
   "metadata": {},
   "source": [
    "### 0. Set up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d924b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system instructions and user prompt\n",
    "with open(\"../prompts/evaluate_diagnostic_reasoning/system_prompt.txt\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "with open(\"../prompts/evaluate_diagnostic_reasoning/user_prompt.txt\") as f:\n",
    "    user_prompt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d4313",
   "metadata": {},
   "source": [
    "### 1. Google Gemini 3 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c772d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the GenAI client\n",
    "google_client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "332b267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str, temperature: float) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # Google Gemini: Make API call and create response object\n",
    "    if model.startswith(\"gemini\"):\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\",  # User prompt with inserted vignette\n",
    "            config=types.GenerateContentConfig(\n",
    "                thinking_config=types.ThinkingConfig(\n",
    "                    thinking_level=\"high\",  # Use thinking_level for Gemini 3, not thinking_budget since it may result in subpar performance\n",
    "                    include_thoughts=True  # Include thought summaries in parts/thought within `response` parameters\n",
    "                    ),\n",
    "                system_instruction=system_prompt,  # System prompt\n",
    "                temperature=temperature  # Model temperature\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Iterate through response object\n",
    "        for part in response.parts:\n",
    "            if not part.text:\n",
    "                continue\n",
    "            if part.thought:\n",
    "                reasoning = part.text  # Extract thought summary\n",
    "            else:\n",
    "                answer = part.text  # Extract differential diagnosis list\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98be9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 134 out of 30 (case 181):   0%|          | 0/1 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating diagnostic reasoning trace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning_subset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (case \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate diagnostic reasoning\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m reasoning, answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_diagnostic_reasoning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoogle_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvignette\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Google advises keeping temperature at 1 for Gemini 3 to avoid messing with reasoning behavior\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save the results to the DataFrame\u001b[39;00m\n\u001b[1;32m     22\u001b[0m reasoning_subset\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_thoughts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m reasoning\n",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m, in \u001b[0;36mgenerate_diagnostic_reasoning\u001b[0;34m(client, model, system_prompt, user_prompt, vignette, temperature)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_diagnostic_reasoning\u001b[39m(client, model: \u001b[38;5;28mstr\u001b[39m, system_prompt: \u001b[38;5;28mstr\u001b[39m, user_prompt: \u001b[38;5;28mstr\u001b[39m, vignette: \u001b[38;5;28mstr\u001b[39m, temperature: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Prepare API call parameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Google Gemini: Make API call and create response object\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<vignette>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvignette\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m</vignette>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# User prompt with inserted vignette\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mthinking_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThinkingConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mthinking_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use thinking_level for Gemini 3, not thinking_budget since it may result in subpar performance\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minclude_thoughts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include thought summaries in parts/thought within `response` parameters\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# System prompt\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Model temperature\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Iterate through response object\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mparts:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/google/genai/models.py:5218\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5216\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5217\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5218\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5219\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[1;32m   5220\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5222\u001b[0m   function_map \u001b[38;5;241m=\u001b[39m _extra_utils\u001b[38;5;241m.\u001b[39mget_function_map(parsed_config)\n\u001b[1;32m   5223\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/google/genai/models.py:4000\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3997\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   3998\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4000\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4001\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   4002\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m   4005\u001b[0m     config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould_return_http_response\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4006\u001b[0m ):\n\u001b[1;32m   4007\u001b[0m   return_value \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentResponse(sdk_http_response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/google/genai/_api_client.py:1388\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1380\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SdkHttpResponse:\n\u001b[1;32m   1385\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m   1386\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m   1387\u001b[0m   )\n\u001b[0;32m-> 1388\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m   response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1390\u001b[0m       response\u001b[38;5;241m.\u001b[39mresponse_stream[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresponse_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1391\u001b[0m   )\n\u001b[1;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, body\u001b[38;5;241m=\u001b[39mresponse_body)\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/google/genai/_api_client.py:1224\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     retry \u001b[38;5;241m=\u001b[39m tenacity\u001b[38;5;241m.\u001b[39mRetrying(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs)\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[0;32m-> 1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/tenacity/__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.1/lib/python3.13/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.1/lib/python3.13/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/google/genai/_api_client.py:1194\u001b[0m, in \u001b[0;36mBaseApiClient._request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m   1191\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m   1192\u001b[0m   )\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1194\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_httpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m   errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_response(response)\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m   1203\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m   1204\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/mh-eval/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.1/lib/python3.13/ssl.py:1285\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.13.1/lib/python3.13/ssl.py:1140\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"gemini-3-pro-preview\"\n",
    "#pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "pbar = tqdm(dataset[dataset['case_id'] == 181].iterrows(), total=1)  # Select specific case for troubleshooting\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(google_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                      1,  # Google advises keeping temperature at 1 for Gemini 3 to avoid messing with reasoning behavior\n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning, answer = generate_diagnostic_reasoning(google_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                      1,  # Google advises keeping temperature at 1 for Gemini 3 to avoid messing with reasoning behavior\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty values in the results because GPT sometimes returns incomplete outputs\n",
    "missing_values = reasoning_subset[\n",
    "    (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "    (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "    (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "    (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"Missing or empty values found in the following cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "else:\n",
    "    print(\"No missing or empty values found in the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c811fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9c85",
   "metadata": {},
   "source": [
    "### 2. OpenAI GPT-5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cccf4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the dataset to avoid overwriting previous results\n",
    "reasoning_subset = pd.read_excel(reasoning_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d04362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # OpenAI: Make API call and create response object\n",
    "    if model.startswith(\"gpt\"):\n",
    "        response = client.responses.create(\n",
    "            model=model,  # gpt-5.2-pro is way too expensive; use gpt-5.2\n",
    "            reasoning={\n",
    "                \"effort\": \"xhigh\",  # Favors even more complete reasoning\n",
    "                \"summary\": \"detailed\"  # Give as much detail as possible in thinking block\n",
    "            },\n",
    "            text={\n",
    "                \"verbosity\": \"low\"  # To keep the model on task for diagnosis\n",
    "            },\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"developer\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # Handle different output array lengths dynamically\n",
    "    if len(response.output) == 1:\n",
    "        # Only differential diagnosis present\n",
    "        reasoning = None\n",
    "        answer = response.output[0].content[0].text\n",
    "    elif len(response.output) >= 2:\n",
    "        # Extract the thought summary by concatenating all thinking blocks using newlines and a list comprehension\n",
    "        reasoning = \"\\n\\n\".join([block.text for block in response.output[0].summary])\n",
    "        \n",
    "        # Extract the differential diagnosis list\n",
    "        answer = response.output[1].content[0].text\n",
    "    else:\n",
    "        # Handle unexpected cases\n",
    "        reasoning = None\n",
    "        answer = None\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fa593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 2 out of 30 (case 62):   3%|▎         | 1/30 [04:05<1:58:28, 245.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 1 out of 30 (case 181).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 3 out of 30 (case 169):   7%|▋         | 2/30 [05:09<1:04:48, 138.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 2 out of 30 (case 62).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 4 out of 30 (case 155):  10%|█         | 3/30 [13:14<2:13:38, 296.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 3 out of 30 (case 169).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 5 out of 30 (case 32):  13%|█▎        | 4/30 [23:04<2:58:51, 412.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 4 out of 30 (case 155).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 6 out of 30 (case 1013):  17%|█▋        | 5/30 [25:59<2:16:09, 326.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 5 out of 30 (case 32).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 7 out of 30 (case 94):  20%|██        | 6/30 [28:18<1:45:11, 262.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 6 out of 30 (case 1013).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 8 out of 30 (case 1032):  23%|██▎       | 7/30 [34:42<1:56:00, 302.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 7 out of 30 (case 94).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 9 out of 30 (case 1003):  27%|██▋       | 8/30 [38:55<1:45:09, 286.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 8 out of 30 (case 1032).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 10 out of 30 (case 80):  30%|███       | 9/30 [43:14<1:37:18, 278.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 9 out of 30 (case 1003).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 11 out of 30 (case 109):  33%|███▎      | 10/30 [46:39<1:25:08, 255.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 10 out of 30 (case 80).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 12 out of 30 (case 118):  37%|███▋      | 11/30 [55:46<1:49:09, 344.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 11 out of 30 (case 109).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 13 out of 30 (case 159):  40%|████      | 12/30 [1:00:37<1:38:32, 328.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 12 out of 30 (case 118).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 14 out of 30 (case 1021):  43%|████▎     | 13/30 [1:09:27<1:50:20, 389.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 13 out of 30 (case 159).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 15 out of 30 (case 104):  47%|████▋     | 14/30 [1:16:21<1:45:48, 396.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 14 out of 30 (case 1021).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 16 out of 30 (case 1049):  50%|█████     | 15/30 [1:19:02<1:21:28, 325.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 15 out of 30 (case 104).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 17 out of 30 (case 115):  53%|█████▎    | 16/30 [1:22:15<1:06:40, 285.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 16 out of 30 (case 1049).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 18 out of 30 (case 1009):  57%|█████▋    | 17/30 [1:23:50<49:30, 228.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 17 out of 30 (case 115).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 19 out of 30 (case 99):  60%|██████    | 18/30 [1:25:29<37:53, 189.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 18 out of 30 (case 1009).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 20 out of 30 (case 23):  63%|██████▎   | 19/30 [1:27:37<31:21, 171.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 19 out of 30 (case 99).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 21 out of 30 (case 1020):  67%|██████▋   | 20/30 [1:32:04<33:20, 200.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 20 out of 30 (case 23).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 22 out of 30 (case 85):  70%|███████   | 21/30 [1:34:01<26:15, 175.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 21 out of 30 (case 1020).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 23 out of 30 (case 1012):  73%|███████▎  | 22/30 [1:39:00<28:16, 212.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 22 out of 30 (case 85).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 24 out of 30 (case 178):  77%|███████▋  | 23/30 [1:41:03<21:37, 185.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 23 out of 30 (case 1012).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 25 out of 30 (case 1047):  80%|████████  | 24/30 [1:42:59<16:28, 164.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 24 out of 30 (case 178).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 26 out of 30 (case 17):  83%|████████▎ | 25/30 [1:48:16<17:31, 210.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 25 out of 30 (case 1047).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 27 out of 30 (case 1033):  87%|████████▋ | 26/30 [1:51:02<13:08, 197.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 26 out of 30 (case 17).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 28 out of 30 (case 1056):  90%|█████████ | 27/30 [1:53:03<08:42, 174.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 27 out of 30 (case 1033).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 29 out of 30 (case 108):  93%|█████████▎| 28/30 [1:56:19<06:01, 180.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 28 out of 30 (case 1056).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 30 out of 30 (case 97):  97%|█████████▋| 29/30 [2:06:14<05:05, 305.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 29 out of 30 (case 108).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 30 out of 30 (case 97): 100%|██████████| 30/30 [2:20:56<00:00, 281.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 30 out of 30 (case 97).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"gpt-5.2\"\n",
    "\n",
    "# Start from specific row if resuming\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(openai_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                    # Temperature not supported with reasoning effort set to high \n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c61c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 1 ===\n",
      "Missing or empty values found in 4 cases:\n",
      "    case_id model_thoughts                                    model_diagnosis\n",
      "4        32                 1. Posttraumatic stress disorder (PTSD) - F43....\n",
      "17     1009                 1. Anorexia nervosa, restricting type - F50.01...\n",
      "25       17                 1. Schizophrenia - ICD-10 F20.9  \\n2. Cannabis...\n",
      "29       97                 1. Bipolar I disorder, current episode manic, ...\n",
      "Rerunning 4 cases with missing data: [4, 17, 25, 29]\n",
      "\n",
      "Rerunning case 4 (case_id: 32)\n",
      "Successfully completed case 4\n",
      "\n",
      "Rerunning case 17 (case_id: 1009)\n",
      "Successfully completed case 17\n",
      "\n",
      "Rerunning case 25 (case_id: 17)\n",
      "Successfully completed case 25\n",
      "\n",
      "Rerunning case 29 (case_id: 97)\n",
      "Successfully completed case 29\n",
      "No missing or empty values found in the results.\n",
      "\n",
      "Completed after 1 iteration(s).\n"
     ]
    }
   ],
   "source": [
    "# Check for missing or empty values and rerun until none remain\n",
    "max_iterations = 10  # Prevent infinite loop\n",
    "iteration = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    # Check for missing or empty values\n",
    "    missing_values = reasoning_subset[\n",
    "        (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "        (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "        (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "        (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "    ]\n",
    "    \n",
    "    if missing_values.empty:\n",
    "        print(\"No missing or empty values found in the results.\")\n",
    "        break\n",
    "    \n",
    "    iteration += 1\n",
    "    print(f\"\\n=== Iteration {iteration} ===\")\n",
    "    print(f\"Missing or empty values found in {len(missing_values)} cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "    \n",
    "    # Get all indices with missing values\n",
    "    missing_indices = missing_values.index.tolist()\n",
    "    print(f\"Rerunning {len(missing_indices)} cases with missing data: {missing_indices}\")\n",
    "    \n",
    "    for index_to_rerun in missing_indices:\n",
    "        print(f\"\\nRerunning case {index_to_rerun} (case_id: {reasoning_subset.iloc[index_to_rerun]['case_id']})\")\n",
    "        \n",
    "        try:\n",
    "            reasoning, answer = generate_diagnostic_reasoning(openai_client,\n",
    "                                                            model,\n",
    "                                                            system_prompt,\n",
    "                                                            user_prompt,\n",
    "                                                            reasoning_subset.iloc[index_to_rerun][\"vignette\"],\n",
    "                                                        )\n",
    "            \n",
    "            reasoning_subset.loc[index_to_rerun, \"model_thoughts\"] = reasoning\n",
    "            reasoning_subset.loc[index_to_rerun, \"model_diagnosis\"] = answer\n",
    "            print(f\"Successfully completed case {index_to_rerun}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing case {index_to_rerun}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nCompleted after {iteration} iteration(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc8cb098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning samples for gpt-5.2 saved to ../../results/evaluate_diagnostic_reasoning/reasoning_samples_gpt-5.2_20251215_213246.json.\n"
     ]
    }
   ],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002dc33f",
   "metadata": {},
   "source": [
    "### 3. Anthropic Claude Opus 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da02d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the dataset to avoid overwriting previous results\n",
    "reasoning_subset = pd.read_excel(reasoning_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590be497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "anthropic_client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # Anthropic Claude: Make API call and create response object\n",
    "    if model.startswith(\"claude\"):\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=20000,  # Max output for Claude Opus 4.5 is 64k but >20k requires streaming\n",
    "            system=system_prompt,\n",
    "            # Extended thinking mode is not compatible with temperature, top_p, or top_k sampling\n",
    "            thinking={\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 19000  # Allocate tokens for thinking - model may not use entire budget\n",
    "            },\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "         # Handle model refusal to answer\n",
    "        if response.stop_reason == \"refusal\":\n",
    "            reasoning = \"N/A\"\n",
    "            answer = \"Model refused to answer the prompt.\"\n",
    "            return reasoning, answer\n",
    "\n",
    "        # Extract the response content\n",
    "        for block in response.content:\n",
    "            if block.type == \"text\":  # Extract differential diagnosis block\n",
    "                answer = block.text\n",
    "            elif block.type == \"thinking\":  # Extract summarized thinking block\n",
    "                reasoning = block.thinking\n",
    "            elif block.type == \"redacted_thinking\":  # Handle redacted thinking block\n",
    "                print(f\"Redacted thinking detected for \\\"{vignette[:30]}...\\\"\")\n",
    "                reasoning = block.thinking\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"claude-opus-4-5-20251101\"\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(anthropic_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                    # Temperature not compatible with extended thinking mode\n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty values in the results because GPT sometimes returns incomplete outputs\n",
    "missing_values = reasoning_subset[\n",
    "    (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "    (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "    (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "    (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"Missing or empty values found in the following cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "else:\n",
    "    print(\"No missing or empty values found in the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46e347",
   "metadata": {},
   "source": [
    "### 4. DeepSeek-V3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50984e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the dataset to avoid overwriting previous results\n",
    "reasoning_subset = pd.read_excel(reasoning_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "deepseek_client = OpenAI(api_key=os.environ.get(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str, temperature: float) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # DeepSeek: Make API call and create response object\n",
    "    if model.startswith(\"deepseek\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\"},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            stream=False\n",
    "        )\n",
    "        # Extract the response content\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        # Extract the thinking block\n",
    "        reasoning = response.choices[0].message.reasoning_content\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90102fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"deepseek-reasoner\"  # Select latest reasoning model; in this case, DeepSeek-V3.2\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(deepseek_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                      0  # DeepSeek recommends temperature 0 for coding/math tasks where there is a correct answer\n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9665cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty values in the results because GPT sometimes returns incomplete outputs\n",
    "missing_values = reasoning_subset[\n",
    "    (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "    (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "    (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "    (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"Missing or empty values found in the following cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "else:\n",
    "    print(\"No missing or empty values found in the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
