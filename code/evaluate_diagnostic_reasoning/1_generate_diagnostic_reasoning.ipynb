{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9e5311",
   "metadata": {},
   "source": [
    "# 1. Generate model thought traces for evaluation of diagnostic reasoning of 4 frontier LLMs (n = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be0d44",
   "metadata": {},
   "source": [
    "## Part A. Load in combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined (literature + synthetic) dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"../../datasets/combined/combined_jama.json\"\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    combined = json.load(f)\n",
    "\n",
    "dataset = pd.DataFrame(combined)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b66d5e",
   "metadata": {},
   "source": [
    "## Part B. Create dataset subset for diagnostic reasoning analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 30 random cases from the combined dataset for diagnostic reasoning analysis and human comparison\n",
    "import random\n",
    "random.seed(1000)  # For reproducibility\n",
    "\n",
    "# Exclude a set of case IDs if needed (seen by clinicians in previous round of evaluation)\n",
    "excluded_case_ids = [48, 81, 90, 142, 166, 168, 176, 177, 44, 43,\n",
    "            113, 114, 131, 132, 152, 156, 21, 27, 50, 89,\n",
    "            110, 123, 153, 175, 1001, 1002, 1010, 1011, 1014, 1019]\n",
    "\n",
    "filtered_combined = [case for case in combined if case['case_id'] not in excluded_case_ids]\n",
    "\n",
    "reasoning_subset = random.sample(filtered_combined, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export reasoning subset as Excel spreadsheet to preserve Unicode characters like quotation marks\n",
    "reasoning_subset = pd.DataFrame(reasoning_subset)\n",
    "reasoning_output_path = \"../../datasets/reasoning/jama_reasoning_subset.xlsx\"\n",
    "\n",
    "reasoning_subset.to_excel(reasoning_output_path, index=False)\n",
    "print(f\"Exported reasoning subset to {reasoning_output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1609e01",
   "metadata": {},
   "source": [
    "## Part C. Set up LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c52fb",
   "metadata": {},
   "source": [
    "### 0. Set up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d924b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system instructions and user prompt\n",
    "with open(\"../prompts/system_prompt.txt\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "with open(\"../prompts/user_prompt.txt\") as f:\n",
    "    user_prompt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d4313",
   "metadata": {},
   "source": [
    "### 1. Google Gemini 3 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the GenAI client\n",
    "google_client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str, temperature: float) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # Google Gemini: Make API call and create response object\n",
    "    if model.startswith(\"gemini\"):\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\",  # User prompt with inserted vignette\n",
    "            config=types.GenerateContentConfig(\n",
    "                thinking_config=types.ThinkingConfig(\n",
    "                    thinking_level=\"high\",  # Use thinking_level for Gemini 3, not thinking_budget since it may result in subpar performance\n",
    "                    include_thoughts=True  # Include thought summaries in parts/thought within `response` parameters\n",
    "                    ),\n",
    "                system_instruction=system_prompt,  # System prompt\n",
    "                temperature=temperature  # Model temperature\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Iterate through response object\n",
    "        for part in response.parts:\n",
    "            if not part.text:\n",
    "                continue\n",
    "            if part.thought:\n",
    "                reasoning = part.text  # Extract thought summary\n",
    "            else:\n",
    "                answer = part.text  # Extract differential diagnosis list\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"gemini-3-pro-preview\"\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(google_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                      1,  # Google advises keeping temperature at 1 for Gemini 3 to avoid messing with reasoning behavior\n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty values in the results because GPT sometimes returns incomplete outputs\n",
    "missing_values = reasoning_subset[\n",
    "    (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "    (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "    (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "    (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"Missing or empty values found in the following cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "else:\n",
    "    print(\"No missing or empty values found in the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c811fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9c85",
   "metadata": {},
   "source": [
    "### 2. OpenAI GPT 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the dataset to avoid overwriting previous results\n",
    "reasoning_subset = pd.read_excel(reasoning_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # OpenAI: Make API call and create response object\n",
    "    if model.startswith(\"gpt\"):\n",
    "        response = client.responses.create(\n",
    "            model=model,  # gpt-5.2-pro is way too expensive; use gpt-5.2\n",
    "            reasoning={\n",
    "                \"effort\": \"xhigh\",  # Favors even more complete reasoning\n",
    "                \"summary\": \"detailed\"  # Give as much detail as possible in thinking block\n",
    "            },\n",
    "            text={\n",
    "                \"verbosity\": \"low\"  # To keep the model on task for diagnosis\n",
    "            },\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # Handle different output array lengths dynamically\n",
    "    if len(response.output) == 1:\n",
    "        # Only differential diagnosis present\n",
    "        reasoning = None\n",
    "        answer = response.output[0].content[0].text\n",
    "    elif len(response.output) >= 2:\n",
    "        # Extract the thought summary by concatenating all thinking blocks using newlines and a list comprehension\n",
    "        reasoning = \"\\n\\n\".join([block.text for block in response.output[0].summary])\n",
    "        \n",
    "        # Extract the differential diagnosis list\n",
    "        answer = response.output[1].content[0].text\n",
    "    else:\n",
    "        # Handle unexpected cases\n",
    "        reasoning = None\n",
    "        answer = None\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a0fa593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 2 out of 30 (case 62):   3%|▎         | 1/30 [02:52<1:23:32, 172.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 1 out of 30 (case 181).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 3 out of 30 (case 169):   7%|▋         | 2/30 [03:55<50:25, 108.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 2 out of 30 (case 62).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 4 out of 30 (case 155):  10%|█         | 3/30 [06:55<1:03:28, 141.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 3 out of 30 (case 169).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 5 out of 30 (case 32):  13%|█▎        | 4/30 [09:18<1:01:18, 141.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 4 out of 30 (case 155).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 6 out of 30 (case 1013):  17%|█▋        | 5/30 [11:26<56:57, 136.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 5 out of 30 (case 32).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 7 out of 30 (case 94):  20%|██        | 6/30 [15:33<1:09:41, 174.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 6 out of 30 (case 1013).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 8 out of 30 (case 1032):  23%|██▎       | 7/30 [19:06<1:11:38, 186.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 7 out of 30 (case 94).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 9 out of 30 (case 1003):  27%|██▋       | 8/30 [21:48<1:05:42, 179.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 8 out of 30 (case 1032).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 10 out of 30 (case 80):  30%|███       | 9/30 [24:52<1:03:08, 180.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 9 out of 30 (case 1003).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 11 out of 30 (case 109):  33%|███▎      | 10/30 [26:58<54:36, 163.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 10 out of 30 (case 80).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 12 out of 30 (case 118):  37%|███▋      | 11/30 [33:11<1:12:06, 227.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 11 out of 30 (case 109).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 13 out of 30 (case 159):  40%|████      | 12/30 [38:44<1:17:54, 259.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 12 out of 30 (case 118).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 14 out of 30 (case 1021):  43%|████▎     | 13/30 [47:06<1:34:26, 333.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 13 out of 30 (case 159).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 15 out of 30 (case 104):  47%|████▋     | 14/30 [50:15<1:17:14, 289.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 14 out of 30 (case 1021).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 16 out of 30 (case 1049):  50%|█████     | 15/30 [51:18<55:20, 221.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 15 out of 30 (case 104).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 17 out of 30 (case 115):  53%|█████▎    | 16/30 [57:15<1:01:10, 262.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 16 out of 30 (case 1049).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 18 out of 30 (case 1009):  57%|█████▋    | 17/30 [1:00:55<54:03, 249.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 17 out of 30 (case 115).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 19 out of 30 (case 99):  60%|██████    | 18/30 [1:02:48<41:40, 208.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 18 out of 30 (case 1009).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 20 out of 30 (case 23):  63%|██████▎   | 19/30 [1:04:43<33:04, 180.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 19 out of 30 (case 99).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 21 out of 30 (case 1020):  67%|██████▋   | 20/30 [1:08:23<32:01, 192.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 20 out of 30 (case 23).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 22 out of 30 (case 85):  70%|███████   | 21/30 [1:09:48<23:59, 159.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 21 out of 30 (case 1020).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 23 out of 30 (case 1012):  73%|███████▎  | 22/30 [1:16:10<30:14, 226.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 22 out of 30 (case 85).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 24 out of 30 (case 178):  77%|███████▋  | 23/30 [1:20:07<26:48, 229.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 23 out of 30 (case 1012).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 25 out of 30 (case 1047):  80%|████████  | 24/30 [1:22:53<21:02, 210.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 24 out of 30 (case 178).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 26 out of 30 (case 17):  83%|████████▎ | 25/30 [1:26:59<18:26, 221.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 25 out of 30 (case 1047).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 27 out of 30 (case 1033):  87%|████████▋ | 26/30 [1:28:57<12:41, 190.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 26 out of 30 (case 17).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 28 out of 30 (case 1056):  90%|█████████ | 27/30 [1:31:08<08:37, 172.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 27 out of 30 (case 1033).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 29 out of 30 (case 108):  93%|█████████▎| 28/30 [1:32:50<05:02, 151.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 28 out of 30 (case 1056).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 30 out of 30 (case 97):  97%|█████████▋| 29/30 [1:36:43<02:55, 175.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 29 out of 30 (case 108).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diagnostic reasoning trace 30 out of 30 (case 97): 100%|██████████| 30/30 [1:40:55<00:00, 201.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed case 30 out of 30 (case 97).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"gpt-5.2\"\n",
    "\n",
    "# Start from specific row if resuming\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(openai_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                    # Temperature not supported with reasoning effort set to high \n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c61c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 1 ===\n",
      "Missing or empty values found in 4 cases:\n",
      "    case_id model_thoughts                                    model_diagnosis\n",
      "4        32                 1. Posttraumatic stress disorder (PTSD) - F43....\n",
      "9        80                 1. Autism Spectrum Disorder (with accompanying...\n",
      "18       99                 1. Bipolar I disorder, current episode manic, ...\n",
      "20     1020                 1. Obsessive-Compulsive Disorder (mixed obsess...\n",
      "Rerunning 4 cases with missing data: [4, 9, 18, 20]\n",
      "\n",
      "Rerunning case 4 (case_id: 32)\n",
      "Successfully completed case 4\n",
      "\n",
      "Rerunning case 9 (case_id: 80)\n",
      "Successfully completed case 9\n",
      "\n",
      "Rerunning case 18 (case_id: 99)\n",
      "Successfully completed case 18\n",
      "\n",
      "Rerunning case 20 (case_id: 1020)\n",
      "Successfully completed case 20\n",
      "\n",
      "=== Iteration 2 ===\n",
      "Missing or empty values found in 1 cases:\n",
      "    case_id model_thoughts                                    model_diagnosis\n",
      "20     1020                 1. Obsessive-Compulsive Disorder (OCD) - F42  ...\n",
      "Rerunning 1 cases with missing data: [20]\n",
      "\n",
      "Rerunning case 20 (case_id: 1020)\n",
      "Successfully completed case 20\n",
      "No missing or empty values found in the results.\n",
      "\n",
      "Completed after 2 iteration(s).\n"
     ]
    }
   ],
   "source": [
    "# Check for missing or empty values and rerun until none remain\n",
    "max_iterations = 10  # Prevent infinite loop\n",
    "iteration = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    # Check for missing or empty values\n",
    "    missing_values = reasoning_subset[\n",
    "        (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "        (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "        (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "        (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "    ]\n",
    "    \n",
    "    if missing_values.empty:\n",
    "        print(\"No missing or empty values found in the results.\")\n",
    "        break\n",
    "    \n",
    "    iteration += 1\n",
    "    print(f\"\\n=== Iteration {iteration} ===\")\n",
    "    print(f\"Missing or empty values found in {len(missing_values)} cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "    \n",
    "    # Get all indices with missing values\n",
    "    missing_indices = missing_values.index.tolist()\n",
    "    print(f\"Rerunning {len(missing_indices)} cases with missing data: {missing_indices}\")\n",
    "    \n",
    "    for index_to_rerun in missing_indices:\n",
    "        print(f\"\\nRerunning case {index_to_rerun} (case_id: {reasoning_subset.iloc[index_to_rerun]['case_id']})\")\n",
    "        \n",
    "        try:\n",
    "            reasoning, answer = generate_diagnostic_reasoning(openai_client,\n",
    "                                                            model,\n",
    "                                                            system_prompt,\n",
    "                                                            user_prompt,\n",
    "                                                            reasoning_subset.iloc[index_to_rerun][\"vignette\"],\n",
    "                                                        )\n",
    "            \n",
    "            reasoning_subset.loc[index_to_rerun, \"model_thoughts\"] = reasoning\n",
    "            reasoning_subset.loc[index_to_rerun, \"model_diagnosis\"] = answer\n",
    "            print(f\"Successfully completed case {index_to_rerun}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing case {index_to_rerun}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nCompleted after {iteration} iteration(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc8cb098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning samples for gpt-5.2 saved to ../../results/evaluate_diagnostic_reasoning/reasoning_samples_gpt-5.2_20251213_191442.json.\n"
     ]
    }
   ],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002dc33f",
   "metadata": {},
   "source": [
    "### 3. Anthropic Claude Opus 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da02d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the dataset to avoid overwriting previous results\n",
    "reasoning_subset = pd.read_excel(reasoning_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590be497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "anthropic_client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # Anthropic Claude: Make API call and create response object\n",
    "    if model.startswith(\"claude\"):\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=20000,  # Max output for Claude Opus 4.5 is 64k but >20k requires streaming\n",
    "            system=system_prompt,\n",
    "            # Extended thinking mode is not compatible with temperature, top_p, or top_k sampling\n",
    "            thinking={\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 19000  # Allocate tokens for thinking - model may not use entire budget\n",
    "            },\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "         # Handle model refusal to answer\n",
    "        if response.stop_reason == \"refusal\":\n",
    "            reasoning = \"N/A\"\n",
    "            answer = \"Model refused to answer the prompt.\"\n",
    "            return reasoning, answer\n",
    "\n",
    "        # Extract the response content\n",
    "        for block in response.content:\n",
    "            if block.type == \"text\":  # Extract differential diagnosis block\n",
    "                answer = block.text\n",
    "            elif block.type == \"thinking\":  # Extract summarized thinking block\n",
    "                reasoning = block.thinking\n",
    "            elif block.type == \"redacted_thinking\":  # Handle redacted thinking block\n",
    "                print(f\"Redacted thinking detected for \\\"{vignette[:30]}...\\\"\")\n",
    "                reasoning = block.thinking\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"claude-opus-4-5-20251101\"\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(anthropic_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                    # Temperature not compatible with extended thinking mode\n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty values in the results because GPT sometimes returns incomplete outputs\n",
    "missing_values = reasoning_subset[\n",
    "    (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "    (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "    (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "    (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"Missing or empty values found in the following cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "else:\n",
    "    print(\"No missing or empty values found in the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46e347",
   "metadata": {},
   "source": [
    "### 4. DeepSeek-V3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50984e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the dataset to avoid overwriting previous results\n",
    "reasoning_subset = pd.read_excel(reasoning_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "deepseek_client = OpenAI(api_key=os.environ.get(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a diagnosis and extract diagnostic reasoning from the thinking blocks\n",
    "def generate_diagnostic_reasoning(client, model: str, system_prompt: str, user_prompt: str, vignette: str, temperature: float) -> tuple:\n",
    "    # Prepare API call parameters\n",
    "    # DeepSeek: Make API call and create response object\n",
    "    if model.startswith(\"deepseek\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt + \"\\n<vignette>\\n\" + vignette + \"\\n</vignette>\"},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            stream=False\n",
    "        )\n",
    "        # Extract the response content\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        # Extract the thinking block\n",
    "        reasoning = response.choices[0].message.reasoning_content\n",
    "\n",
    "    return reasoning, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90102fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reasoning samples and generate diagnostic reasoning, saving the results\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"deepseek-reasoner\"  # Select latest reasoning model; in this case, DeepSeek-V3.2\n",
    "pbar = tqdm(reasoning_subset.iterrows(), total=reasoning_subset.shape[0])  # Progress bar for tracking\n",
    "\n",
    "for index, row in pbar:\n",
    "    # Update progress bar description with current case ID\n",
    "    pbar.set_description(f\"Generating diagnostic reasoning trace {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']})\")\n",
    "\n",
    "    # Generate diagnostic reasoning\n",
    "    reasoning, answer = generate_diagnostic_reasoning(deepseek_client,\n",
    "                                                      model,\n",
    "                                                      system_prompt,\n",
    "                                                      user_prompt,\n",
    "                                                      row[\"vignette\"],\n",
    "                                                      0  # DeepSeek recommends temperature 0 for coding/math tasks where there is a correct answer\n",
    "                                                    )\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    reasoning_subset.loc[index, \"model_thoughts\"] = reasoning\n",
    "    reasoning_subset.loc[index, \"model_diagnosis\"] = answer\n",
    "    print(f\"Completed case {index + 1} out of {reasoning_subset.shape[0]} (case {row['case_id']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9665cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty values in the results because GPT sometimes returns incomplete outputs\n",
    "missing_values = reasoning_subset[\n",
    "    (reasoning_subset[\"model_thoughts\"].isnull()) | \n",
    "    (reasoning_subset[\"model_thoughts\"] == \"\") |\n",
    "    (reasoning_subset[\"model_diagnosis\"].isnull()) | \n",
    "    (reasoning_subset[\"model_diagnosis\"] == \"\")\n",
    "]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"Missing or empty values found in the following cases:\")\n",
    "    print(missing_values[[\"case_id\", \"model_thoughts\", \"model_diagnosis\"]])\n",
    "else:\n",
    "    print(\"No missing or empty values found in the results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp\n",
    "import datetime\n",
    "\n",
    "# Save to a JSON file\n",
    "results_path = f\"../../results/evaluate_diagnostic_reasoning/reasoning_samples_{model}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "reasoning_subset.to_json(results_path, orient=\"records\", indent=2)\n",
    "print(f\"Reasoning samples for {model} saved to {results_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
