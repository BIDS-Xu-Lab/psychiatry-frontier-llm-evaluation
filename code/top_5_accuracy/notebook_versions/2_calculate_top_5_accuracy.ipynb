{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83447dd6",
   "metadata": {},
   "source": [
    "# 2. Calculate top-5 accuracy from generated diagnoses via LLM judge (GPT-5-mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72270adc",
   "metadata": {},
   "source": [
    "## A. Extract true diagnoses and predicted diagnoses from output/results files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5284ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: Parse ground truth diagnoses and model-predicted diagnoses strings from DataFrame into lists\n",
    "import re\n",
    "\n",
    "def parse_ground_truth_diagnoses(diagnosis_str) -> list:\n",
    "    \"\"\"\n",
    "    Converts '1. Diagnosis A\\n2. Diagnosis B' into ['Diagnosis A', 'Diagnosis B']\n",
    "    \"\"\"\n",
    "    # Handle empty or non-string inputs\n",
    "    if not isinstance(diagnosis_str, str): return []\n",
    "\n",
    "    # Check if the diagnosis string is a numbered list (starts with \"1.\" or similar)\n",
    "    if re.search(r'^\\d+\\.', diagnosis_str.strip()):\n",
    "        # Split by newline and map each diagnosis into a list\n",
    "        diagnoses = []\n",
    "        for line in diagnosis_str.strip().split('\\n'):\n",
    "            # Remove the numbering and any leading/trailing whitespace\n",
    "            match = re.match(r'\\d+\\.\\s+(.*)', line)  # Regex to capture text after numbering\n",
    "            if match:\n",
    "                diagnoses.append(match.group(1).strip())  # Add the cleaned diagnosis to the list\n",
    "            else:\n",
    "                diagnoses.append(line.strip())  # If not numbered, just add the line as-is\n",
    "        return diagnoses\n",
    "    else:\n",
    "        # If not a numbered list, split by semicolons\n",
    "        diagnoses = re.split(r';', diagnosis_str)\n",
    "        return [diag.strip() for diag in diagnoses if diag.strip()]\n",
    "\n",
    "def parse_model_predicted_diagnoses(model_diagnoses_str) -> list:\n",
    "    \"\"\"\n",
    "    Converts '1. Diagnosis A\\n2. Diagnosis B' into ['Diagnosis A', 'Diagnosis B']\n",
    "    \"\"\"\n",
    "    # Handle empty or non-string inputs\n",
    "    if not isinstance(model_diagnoses_str, str): return []\n",
    "\n",
    "    diagnoses = []\n",
    "    for line in model_diagnoses_str.strip().split('\\n'):\n",
    "        # Remove the numbering and any leading/trailing whitespace\n",
    "        match = re.match(r'\\d+\\.\\s+(.*)', line)\n",
    "        if match:\n",
    "            diagnoses.append(match.group(1).strip())\n",
    "        else:\n",
    "            # If not numbered, just add the line\n",
    "            diagnoses.append(line.strip())\n",
    "    return diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ground truth and predicted diagnoses using hybrid fuzzy + LLM approach for one case\n",
    "from rapidfuzz import fuzz\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "\n",
    "# Load API key from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define hybrid evaluator class for one case\n",
    "class HybridEvaluator:\n",
    "    def __init__(self, fuzzy_threshold=90, llm_model=\"gpt-5-mini\"):\n",
    "        self.fuzzy_threshold = fuzzy_threshold\n",
    "        self.llm_model = llm_model\n",
    "        # The cache prevents paying for the same comparison twice\n",
    "        # Structure: {\"True Term || Pred Term\": True/False}\n",
    "        self.cache = {} \n",
    "        self.llm_calls = 0\n",
    "\n",
    "    def check_match(self, true_diag, pred_diag):\n",
    "        \"\"\"\n",
    "        Returns True if match, False if not.\n",
    "        Uses Fuzzy first, then falls back to LLM.\n",
    "        \"\"\"\n",
    "        # 1. Normalize strings\n",
    "        t = true_diag.lower().strip()\n",
    "        p = pred_diag.lower().strip()\n",
    "        \n",
    "        # 2. TIER 1: Fuzzy String Matching (Free & Fast)\n",
    "        # token_set_ratio handles reordering (e.g. \"Type 2 Diabetes\" == \"Diabetes Type 2\")\n",
    "        fuzzy_score = fuzz.token_set_ratio(t, p)\n",
    "        if fuzzy_score >= self.fuzzy_threshold:\n",
    "            return True\n",
    "\n",
    "        # 3. TIER 2: LLM Judge (Semantic)\n",
    "        # Only runs if fuzzy score is low (e.g., < 95)\n",
    "        # Check cache first\n",
    "        cache_key = f\"{t} || {p}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        # Call LLM\n",
    "        #print(f\"Fuzzy threshold exceeded. Invoking LLM for: '{t}' vs '{p}'\")\n",
    "        is_match = self._ask_llm(t, p)\n",
    "        \n",
    "        # Update Cache\n",
    "        self.cache[cache_key] = is_match\n",
    "        self.llm_calls += 1\n",
    "        return is_match\n",
    "    \n",
    "\n",
    "    def _ask_llm(self, t, p):\n",
    "        # Define prompt for LLM-as-a-judge\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "        Your task is to act as a strict medical adjudicator specializing in psychiatry and identify whether the predicted diagnosis is clinically equivalent to (or a valid subclass of) the true diagnosis. Your standards are exacting, and you must consider the nuances of each diagnosis carefully. As much as possible, adhere to the diagnostic language laid out in the DSM-5-TR, and utilize the included ICD-10 F-codes to aid your determination.\n",
    "        \n",
    "        True Diagnosis: \"{t}\"\n",
    "        Predicted Diagnosis: \"{p}\"\n",
    "        \n",
    "        Return JSON ONLY: {{ \"match\": <true/false> }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Define response schema\n",
    "        class DiagnosisMatch(BaseModel):\n",
    "            match: bool # True if match, False if not\n",
    "        \n",
    "        try:\n",
    "            response = client.responses.parse(\n",
    "                model=\"gpt-5-mini\",\n",
    "                input=[\n",
    "                    {\n",
    "                          \"role\": \"user\",\n",
    "                          \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                text_format=DiagnosisMatch,\n",
    "            )\n",
    "            result = json.loads(response.output[1].content[0].text)\n",
    "            return result.get(\"match\", False)  # Default to False if key missing\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af233e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cases from JSON to Pandas DataFrame\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_results_path = \"../../../results/top_5_accuracy/predicted_diagnoses/\"\n",
    "models = os.listdir(model_results_path)\n",
    "results_path = model_results_path + models[0]\n",
    "\n",
    "with open(results_path, 'r') as f:\n",
    "    cases = json.load(f)\n",
    "\n",
    "cases_df = pd.DataFrame(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6ffadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for predicted_diagnoses_claude-opus-4-5-20251101_20251215_225418.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [2:18:56<00:00, 42.54s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Made 1303 calls to LLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "COL_TRUE = 'diagnosis' \n",
    "COL_PRED = 'model_diagnosis'\n",
    "\n",
    "# Initialize Evaluator\n",
    "evaluator = HybridEvaluator(fuzzy_threshold=90, llm_model=\"gpt-5-mini\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate through DataFrame\n",
    "print(f\"Starting evaluation for {models[0]}...\")\n",
    "for index, row in tqdm(cases_df.iterrows(), total=len(cases_df)):\n",
    "    \n",
    "    # 1. Parse Data\n",
    "    y_true = parse_ground_truth_diagnoses(row[COL_TRUE])\n",
    "    y_pred = parse_model_predicted_diagnoses(row[COL_PRED])\n",
    "    \n",
    "    # If no ground truth, skip\n",
    "    if not y_true:\n",
    "        continue\n",
    "\n",
    "    # 2. Analyze Matches\n",
    "    # We map which TRUE diagnoses were found in the PRED list\n",
    "    found_indices = set()\n",
    "    first_match_rank = None # For MRR\n",
    "    \n",
    "    # Iterate through predictions (Order matters for Rank!)\n",
    "    for rank_idx, pred_item in enumerate(y_pred):\n",
    "        current_rank = rank_idx + 1 # 1-based rank\n",
    "        \n",
    "        # Check against ALL true items\n",
    "        is_this_pred_correct = False\n",
    "        \n",
    "        for true_idx, true_item in enumerate(y_true):\n",
    "            # THE HYBRID CHECK\n",
    "            if evaluator.check_match(true_item, pred_item):\n",
    "                is_this_pred_correct = True\n",
    "                found_indices.add(true_idx)\n",
    "                \n",
    "        # If this prediction was a match, and it's the first one we've seen...\n",
    "        if is_this_pred_correct and first_match_rank is None:\n",
    "            first_match_rank = current_rank\n",
    "\n",
    "    # 3. Calculate Metrics\n",
    "    \n",
    "    # Hybrid Recall@5: % of true diagnoses found\n",
    "    recall_score = len(found_indices) / len(y_true)\n",
    "    \n",
    "    # Hybrid Hit Rate: Did we find at least one?\n",
    "    hit_rate = 1.0 if len(found_indices) > 0 else 0.0\n",
    "    \n",
    "    # Hybrid MRR: 1 / Rank of first match\n",
    "    mrr_score = (1 / first_match_rank) if first_match_rank else 0.0\n",
    "    \n",
    "    # Hybrid Top-1: Did the very first prediction match *any* truth?\n",
    "    # We can check if Rank 1 was the first match\n",
    "    top1_score = 1.0 if first_match_rank == 1 else 0.0\n",
    "\n",
    "    results.append({\n",
    "        \"case_id\": row['case_id'],\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"hybrid_top1\": top1_score,\n",
    "        \"hybrid_hit_rate\": hit_rate,\n",
    "        \"hybrid_recall\": recall_score,\n",
    "        \"hybrid_mrr\": mrr_score\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([cases_df.reset_index(drop=True), results_df], axis=1)\n",
    "\n",
    "print(f\"Done! Made {evaluator.llm_calls} calls to LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e331c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL DIAGNOSTIC PERFORMANCE (Mean Scores)===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bf082\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bf082_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_bf082_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bf082_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bf082_row0_col0\" class=\"data row0 col0\" >Top-1 Accuracy</td>\n",
       "      <td id=\"T_bf082_row0_col1\" class=\"data row0 col1\" >64.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bf082_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bf082_row1_col0\" class=\"data row1 col0\" >Top-5 Hit Rate</td>\n",
       "      <td id=\"T_bf082_row1_col1\" class=\"data row1 col1\" >80.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bf082_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bf082_row2_col0\" class=\"data row2 col0\" >Recall@5</td>\n",
       "      <td id=\"T_bf082_row2_col1\" class=\"data row2 col1\" >74.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bf082_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bf082_row3_col0\" class=\"data row3 col0\" >MRR</td>\n",
       "      <td id=\"T_bf082_row3_col1\" class=\"data row3 col1\" >71.76%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x129b3d940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Aggregate Statistics\n",
    "stats = {\n",
    "    \"Metric\": [\"Top-1 Accuracy\", \"Top-5 Hit Rate\", \"Recall@5\", \"MRR\"],\n",
    "    \"Score\": [\n",
    "        results_df['hybrid_top1'].mean(),\n",
    "        results_df['hybrid_hit_rate'].mean(),\n",
    "        results_df['hybrid_recall'].mean(),\n",
    "        results_df['hybrid_mrr'].mean()\n",
    "    ]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "# Display nicely formatted percentages\n",
    "print(\"\\n=== FINAL DIAGNOSTIC PERFORMANCE (Mean Scores) ===\")\n",
    "stats_df.style.format({\"Score\": \"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d188ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Cases Completely Missed: 38\n",
      "Example Miss:\n",
      "diagnosis             Anorexia nervosa with focal cortical dysplasia\n",
      "model_diagnosis    1. Anorexia Nervosa, Restricting Type - F50.01...\n",
      "Name: 1, dtype: object\n",
      "\n",
      "Saved detailed results to 'predicted_diagnoses_claude-opus-4-5-20251101_20251215_225418.jsondiagnostic_evaluation_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# 2. Inspecting Failures\n",
    "# Return rows where Hit Rate was 0 (Total Misses)\n",
    "misses = final_df[final_df['hybrid_hit_rate'] == 0]\n",
    "print(f\"\\nTotal Cases Completely Missed: {len(misses)}\")\n",
    "if len(misses) > 0:\n",
    "    print(\"Example Miss:\")\n",
    "    print(misses[[COL_TRUE, COL_PRED]].iloc[0])\n",
    "\n",
    "# 3. Export to CSV\n",
    "final_df.to_csv(f\"{models[0]}diagnostic_evaluation_results.csv\", index=False)\n",
    "print(f\"\\nSaved detailed results to '{models[0]}diagnostic_evaluation_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
